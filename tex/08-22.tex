\section{08-22}
Information theory studies the processing, quantification, storage, and communication
of information.
\begin{itemize}
    \item 1948 --- Claude Shannon defines \emph{Shannon Entropy} in ``The Mathematical
          Theory of Communication.'' Answers questions:
          \begin{enumerate}
              \item What is information?
              \item How do we quantify information?
              \item How do we transmit information?
          \end{enumerate}
    \item 2001 --- Shannon Award is created, with Shannon the first recipient.
    \item 1900 --- Max Plank describes Black-body Radiation
    \item 1920s --- Heisenberg, Bohr, and Schrödinger, Matrix Mechanics
    \item 1930s --- Hilbert, Dirac, Von Neumann describe the Hilbert Space, Mathematical
          foundation of Quantum Mechanics, and Von Neumann Entropy
    \item Interaction: Quantum Information
    \item 1950s -- 1970s --- Mathematical Quantities of Information
    \item 1970s
          \begin{itemize}
              \item Information Transmission by Coherent Laser
              \item Alexander Holevo --- Holevo Bound
                    \begin{itemize}
                        \item 1998 --- Holevo et al show bound is tight (receive 2017 Shannon Award)
                    \end{itemize}
          \end{itemize}
    \item 1980s --- Richard Feynman: Computing with Quantum Mechanical Model
    \item 1990s --- Peter Schor: Quantum Algorithm for Prime Factorization
          \begin{itemize}
              \item In general, the only known algorithm for determining the prime factors
                    of a number is naïve factorization. For example, given
                    $n = 4801 \times 35317 = 169556917$, to retrieve the factors
                    4801 and 35317 requires substantially more time than to simply
                    construct the number via multiplication.
          \end{itemize}
    \item
\end{itemize}